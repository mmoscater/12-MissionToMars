{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission To Mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Scraping\n",
    "\n",
    "Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "\n",
    "\n",
    "* Create a Jupyter Notebook file called `mission_to_mars.ipynb` and use this to complete all of your scraping and analysis tasks. The following outlines what you need to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News:\n",
    "\n",
    "* Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA's InSight Takes Its First Selfie\n",
      "Two new image mosaics detail the lander's deck and \"workspace\" — the surface where it will eventually set down its science instruments.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# get first title\n",
    "news_title = soup.find('div',class_=\"content_title\").text\n",
    "# get first paragraph\n",
    "news_p = soup.find('div',class_=\"article_teaser_body\").text\n",
    "\n",
    "print(news_title)\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "* Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "* Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "time.sleep(1)\n",
    "browser.find_by_id('full_image').first.click()\n",
    "#browser.implicitly_wait(10)\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_href('details')\n",
    "time.sleep(1)\n",
    "img_html = browser.html\n",
    "img_soup = bs(img_html,'html.parser')\n",
    "# print(img_soup.prettify())\n",
    "featured_image_url = 'http://jpl.nasa.gov' + img_soup.find('img',class_='main_image')['src']\n",
    "#print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Weather\n",
    "\n",
    "* Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_wthr_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(mars_wthr_url)\n",
    "\n",
    "time.sleep(1)\n",
    "mars_wthr_html = browser.html\n",
    "twtr_soup = bs(mars_wthr_html,'html.parser')\n",
    "# print(twtr_soup.prettify())\n",
    "tweets = twtr_soup.find_all('li',{'class':'js-stream-item stream-item stream-item '})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@rtphokie\n",
      "That’s no snowman, it’s Saturn’s moons Dione (top) and Rhea (bottom) https://www.jpl.nasa.gov/spaceimages/details.php?id=PIA12728 …pic.twitter.com/5w86w3JGE2\n",
      "@MarsWxReport\n",
      "Sol 2256 (2018-12-11), high -11C/12F, low -70C/-93F, pressure at 8.43 hPa, daylight 06:36-18:50\n"
     ]
    }
   ],
   "source": [
    "# find first/latest weather tweet (to move past retweeted posts)\n",
    "for tweet in tweets:\n",
    "    content_u = tweet.find('span',{'class':'username u-dir u-textTruncate'}).text\n",
    "    mars_weather = tweet.find('p').text\n",
    "    print(content_u)\n",
    "    print(mars_weather)\n",
    "    # find original post\n",
    "    if content_u == '@MarsWxReport':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sol 2256 (2018-12-11), high -11C/12F, low -70C/-93F, pressure at 8.43 hPa, daylight 06:36-18:50\n"
     ]
    }
   ],
   "source": [
    "# validate\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts\n",
    "\n",
    "* Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "* Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres\n",
    "\n",
    "* Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "* Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "* Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "* Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "* Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "  * Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "* Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data.\n",
    "\n",
    "* Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "* Use Splinter to navigate the sites when needed and BeautifulSoup to help find and parse out the necessary data.\n",
    "\n",
    "* Use Pymongo for CRUD applications for your database. For this homework, you can simply overwrite the existing document each time the `/scrape` url is visited and new data is obtained.\n",
    "\n",
    "* Use Bootstrap to structure your HTML template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
