{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission To Mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Scraping\n",
    "\n",
    "Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "\n",
    "\n",
    "* Create a Jupyter Notebook file called `mission_to_mars.ipynb` and use this to complete all of your scraping and analysis tasks. The following outlines what you need to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set exec path for chromedriver\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News:\n",
    "\n",
    "* Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "\n",
    "# get html from current window\n",
    "html = browser.html\n",
    "\n",
    "# make soup\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# get first title\n",
    "news_title = soup.find('div',class_=\"content_title\").text\n",
    "\n",
    "# get first paragraph\n",
    "news_p = soup.find('div',class_=\"article_teaser_body\").text\n",
    "\n",
    "# verify\n",
    "print(news_title)\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "* Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "* Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set jpl url\n",
    "jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "# navigate to url\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "# add wait for good measure..\n",
    "time.sleep(1)\n",
    "\n",
    "# find the image\n",
    "browser.find_by_id('full_image').first.click()\n",
    "\n",
    "# another wait...\n",
    "time.sleep(2)\n",
    "\n",
    "# click details link\n",
    "browser.click_link_by_partial_href('details')\n",
    "\n",
    "# wait some more...\n",
    "time.sleep(1)\n",
    "\n",
    "# set current browser html\n",
    "img_html = browser.html\n",
    "\n",
    "# make soup\n",
    "img_soup = bs(img_html,'lxml')\n",
    "# print(img_soup.prettify())\n",
    "\n",
    "# set image url\n",
    "featured_image_url = 'http://jpl.nasa.gov' + img_soup.find('img',class_='main_image')['src']\n",
    "#print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Weather\n",
    "\n",
    "* Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mars weather twitter url\n",
    "mars_wthr_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "# visit url\n",
    "browser.visit(mars_wthr_url)\n",
    "\n",
    "# add wait to let page load\n",
    "time.sleep(1)\n",
    "\n",
    "# grab current window html\n",
    "mars_wthr_html = browser.html\n",
    "\n",
    "# make soup\n",
    "twtr_soup = bs(mars_wthr_html,'html.parser')\n",
    "\n",
    "# print(twtr_soup.prettify())\n",
    "\n",
    "# find tweets\n",
    "tweets = twtr_soup.find_all('li',{'class':'js-stream-item stream-item stream-item '})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find first/latest weather tweet (to move past retweeted posts)\n",
    "for tweet in tweets:\n",
    "    # set content user\n",
    "    content_u = tweet.find('span',{'class':'username u-dir u-textTruncate'}).text\n",
    "    \n",
    "    # set weather tweet\n",
    "    mars_weather = tweet.find('p').text\n",
    "    \n",
    "    # see results\n",
    "    print(content_u)\n",
    "    print(mars_weather)\n",
    "    \n",
    "    # find original post and break when found\n",
    "    if content_u == '@MarsWxReport':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts\n",
    "\n",
    "* Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "* Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url for pd scrape\n",
    "mars_facts_url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read html with pandas\n",
    "fact_tbls = pd.read_html(mars_facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "fact_tbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check type\n",
    "type(fact_tbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to df\n",
    "fact_df = fact_tbls[0]\n",
    "\n",
    "# set headers\n",
    "fact_df.columns = ['Attribute','Value']\n",
    "\n",
    "# set index in place\n",
    "fact_df.set_index('Attribute',inplace=True)\n",
    "fact_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to html table\n",
    "fact_df_html = fact_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip new lines chars\n",
    "cln_fact_df_html = fact_df_html.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check table\n",
    "cln_fact_df_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "fact_df.to_html('table.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "!open table.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres\n",
    "\n",
    "* Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mars' hemispheres.\n",
    "\n",
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "* Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "* Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mars hemisphere url\n",
    "mars_hem_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# browser load and scrape\n",
    "browser.visit(mars_hem_url)\n",
    "\n",
    "# add wait for safe measures\n",
    "time.sleep(1)\n",
    "\n",
    "# make some soup\n",
    "hem_soup = bs(browser.html,'lxml')\n",
    "\n",
    "# find all links\n",
    "thumb_list = hem_soup.find_all('div',{'class':'description'})\n",
    "\n",
    "# verify\n",
    "thumb_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set partial link (prefix)\n",
    "part_link = 'https://astrogeology.usgs.gov'\n",
    "\n",
    "# create empty list\n",
    "mars_hems = []\n",
    "\n",
    "# loop through thumbnail list\n",
    "for t in thumb_list:\n",
    "    # set link text variable\n",
    "    lnk = t.find('h3').text\n",
    "    \n",
    "    # print to see which hem is clicking...\n",
    "    print(f'>>>>>searching for *{lnk}*<<<<<')\n",
    "    \n",
    "    # click link with text\n",
    "    browser.click_link_by_partial_text(lnk)\n",
    "    \n",
    "    # make temp soup\n",
    "    temp_soup = bs(browser.html,'lxml')\n",
    "    \n",
    "    # assign temp variables from soup search\n",
    "    img = temp_soup.find('img','wide-image')\n",
    "    div2 = temp_soup.find('div','content')\n",
    "    \n",
    "    # create dict\n",
    "    t_dict = {'img_url':part_link + img['src'],\n",
    "              'title':div2.h2.text}\n",
    "    \n",
    "    # check dict\n",
    "    print(t_dict)\n",
    "    \n",
    "    # add to list\n",
    "    mars_hems.append(t_dict)\n",
    "    \n",
    "    # go back\n",
    "    browser.click_link_by_partial_text('Back')\n",
    "\n",
    "# verify list\n",
    "mars_hems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "* Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "* Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "  * Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "* Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data.\n",
    "\n",
    "* Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "* Use Splinter to navigate the sites when needed and BeautifulSoup to help find and parse out the necessary data.\n",
    "\n",
    "* Use Pymongo for CRUD applications for your database. For this homework, you can simply overwrite the existing document each time the `/scrape` url is visited and new data is obtained.\n",
    "\n",
    "* Use Bootstrap to structure your HTML template."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
